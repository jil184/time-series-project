---
title: "The Report"
author: "Group 6"
date: "2024-05-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fpp3)
library(grid)
library(gridExtra)
library(forecast)
library(tseries)
```

# 1. Exploratory data analysis

The data set contains information about the quarterly (chain volume) gross domestic product (QGDP) for all ANZSIC06 industry groups in New Zealand, measured in the prices from 2009/10 in NZD Millions from 1987 Q2 until 2021 Q4. Our task is exploring the quarterly GDP in Local Government Administration group. 

```{r}
# read-in data
train <- read_csv("qgdp_training.csv",show_col_types = FALSE)
# convert data into tsibble
train <- train %>% mutate(Quarter = yearquarter(Date)) %>%  
  select(Quarter,`Local Government Administration`) %>% 
  as_tsibble(index = Quarter)
```
Firstly, we look at time plot and relevant plots.

```{r}
# time plot
train %>% autoplot(`Local Government Administration`) + ggtitle(" GDP in Local Government Administration") + 
  xlab("Time") + ylab("NZD (Millions)") +
  theme_bw()+ 
  theme(plot.title = element_text(hjust = 0.5)) 
# Seasonal subseries plot with gg_subseries
train %>%
  gg_subseries(`Local Government Administration`) +
  ylab("NZD(Millions)")
# correlogram
train %>%
  ACF(`Local Government Administration`, lag_max = 50) %>%
  autoplot() 
```

Looking at the time plot, we observed that this is a trending with structure break and seasonality time series. More specific, the average GDP for Q4 was the highest value while the average GDP for Q1 was the smallest value based on the sub-series plot. We can also see an increasing year-on-year trend for each quarter in the sub-series plot and the GDP, on average, increases as the year increases. 
Also, we can see the autocorrelations for small lags tend to be large and positive and decays slowly as the lags increases and the seasonal pattern is not clear in the correlogram. Also, noticing the reverse pattern appeared after lag 43, suggesting the structure break is in this trended time series. 

Next, we have a look at the decomposition plot by using STL decomposition. We decide to use STL model because it allows the seasonal component change over time and we see the magnitude of variation around the trend-cycle does not really vary with the level, so we only consider additive decomposition.   

```{r}
# decomposition
stl.dcmp <- train %>%
  model(STL(`Local Government Administration`,robust = TRUE)) %>%
  components() 
stl.dcmp%>% 
  autoplot()
```

Based on the decomposition result, we can see the trend component dominates the time series, accounting for most of the variability. This is followed by the remainder component, and then the seasonal component. There is a period of time the trend is negative and then stable (from 1990 Q1 until around year of 2003). However, the long-term trend appears to be positive, indicating the GDP in Local Government Administration are increasing. The remainder component appears random, with no discernible patterns, and may be consistent with white noise but we would need further analysis test this. The seasonal component changes quickly over time, where the magnitudes are significantly increasing.



# 2. ETS models


Based on the analysis in the first step, we can confirm that there are indeed trends and seasonal changes, but I am currently unable to determine whether to use multiplicative seasonality or additive seasonality to fit the ets model. And it is necessary to consider whether to adopt a pounded trend. So, I decided to manually try out the pros and cons of these four models first.

```{r}
data <- read_csv("qgdp_training.csv",show_col_types = FALSE)

data <- data %>% mutate(Quarter = yearquarter(Date)) %>%  
  select(Quarter,`Local Government Administration`) %>% 
  as_tsibble(index = Quarter)

fit <- data %>%
       model(
         additive_season = ETS(`Local Government Administration` ~ error("A") + trend("A") + season("A")),
         multiplicative_season = ETS(`Local Government Administration` ~ error("M") + trend("A") + season("M")),
         additive_season_damped = ETS(`Local Government Administration` ~ error("A") + trend("Ad") + season("A")),
         multiplicative_season_damped = ETS(`Local Government Administration` ~ error("M") + trend("Ad") + season("M"))
       )

fc <- fit %>% forecast(h = "5 years")
fc %>% autoplot(data)

# Check model summary
report(fit)

# Use the method of automatically selecting the best model to check if we have selected it correctly.
fit_best <- data %>%
  model(ETS(`Local Government Administration`))
report(fit_best)
```

We can clearly see here that among many aspects of error data, the method of adding seasonality and with Damped trend has the smallest errors, and my method of automatically obtaining the minimum AIC also told me that I should choose the method of adding seasonality and with Damped trend.

## ETS Model Introduction

The ETS (Error, Trend, Seasonality) model we selected is specified as an additive error, damped additive trend, and additive seasonality model. Below are the equations representing this model setup:

### Model Equations

The ETS model equations for an additive error, damped additive trend, and additive seasonality are as follows:

- **Forecast equation:**
  \[
  \hat{y}_{t|t-1} = (l_{t-1} + \phi b_{t-1}) + s_{t-m}
  \]

- **Level equation:**
  \[
  l_t = l_{t-1} + \phi b_{t-1} + \alpha e_t
  \]

- **Trend equation:**
  \[
  b_t = \phi b_{t-1} + \beta e_t
  \]

- **Seasonal equation:**
  \[
  s_t = s_{t-m} + \gamma e_t
  \]

Where:
- \( \hat{y}_{t|t-1} \) is the one-step ahead forecast,
- \( l_t \) is the level component at time \( t \),
- \( b_t \) is the trend component at time \( t \),
- \( s_t \) is the seasonal component at time \( t \),
- \( \phi \) is the damping factor,
- \( \alpha, \beta, \gamma \) are the smoothing parameters,
- \( e_t \) is the forecast error at time \( t \).



# 3. ARIMA models

To fit an appropriate ARIMA model, we first need to test the stationarity of the time series data. Because if the time series data is stationary, it means that its statistical characteristics are constant over time, allowing for better modeling and prediction. So here, we first conducted ADF test on the data.

```{r}
# Load the data.
data <- read.csv("qgdp_training.csv")

# Check the stationarity of the data.
adf.test(data$Local.Government.Administration)
```

From the test results, we can see that the p-value is greater than 0.05/0.01 (commonly used significance level), indicating that we cannot reject the null hypothesis that the data is non-stationary. Therefore, we need to perform differencing on the data.

```{r}
# Perform differencing on the data.
diff_data <- diff(data$Local.Government.Administration)

# Check the stability of the data.
adf.test(diff_data)

# Draw a time series plot after one differencing.
plot(diff_data, main="Data after one differencing", ylab="Differencing Value", xlab="Time")
```

After conducting a differencing analysis, although the p-value was much less than 0.05, I encountered an error when manually fitting the ARIMA model to the data after one differencing analysis. It reminds me that there are still unstable parts in the data. At the same time, I drew a time series plot of the differencing data, and it can be observed that the data did not tend to be stable, but still showed a certain trend. So I chose to perform a second differencing.

```{r}
# Perform differencing secondly on the data.
diff2_data <- diff(diff_data)

# Check the stability of the data.
adf.test(diff2_data)

# Draw a time series plot after two differencing.
plot(diff2_data, main="Data after two differencing", ylab="Differencing Value", xlab="Time")
```

From the results, we can see that the p-value of the data after the second differencing is still very small (less than 0.05). And by drawing a time series plot of the differencing data, we can see that the data has become more stable, so I decided to start fitting the ARIMA model. Firstly, manual fitting is performed. In order to determine the p-value and q-value of the model, we first draw the ACF and PACF plots of the data after the second differencing.

```{r}
# Draw the ACF and PACF plots.
acf(diff2_data)
pacf(diff2_data)
```

Through the ACF plot, we can locate the p-value candidate values as 1, 2, or 4. Because the ACF plot is significantly truncated after these lag values . Through the PACF plot, we can locate the p-value candidate values as 1, 3, or 7. Because the PACF plot is significantly truncated after these lag values. Next, we create a shortlist of appropriate candidate ARIMA models by arranging and combining different q and p values.

```{r}
# Fitting the ARIMA model manually.
model1 <- arima(data$Local.Government.Administration, order=c(1,2,1))
model2 <- arima(data$Local.Government.Administration, order=c(2,2,1))
model3 <- arima(data$Local.Government.Administration, order=c(4,2,1))
model4 <- arima(data$Local.Government.Administration, order=c(4,2,3))
model5 <- arima(data$Local.Government.Administration, order=c(7,2,1))
model6 <- arima(data$Local.Government.Administration, order=c(7,2,3))

# Fitting the ARIMA model automatically.
auto_model <- auto.arima(data$Local.Government.Administration)

# Compare the AIC value of the models.
AIC(model1, model2, model3, model4, model5, model6, auto_model)
```

By comparing the AIC values of the models, we can find that Model 6 has the lowest AIC value. It performs best in balancing goodness of fit and model complexity. We can preliminarily confirm that it has the best predictive ability. To further validate our idea, I checked its residual ACF plot and performed Ljung-Box test.

```{r}
# Check the residuals of the model6 and do the Ljung-Box.
acf(residuals(model6))
Box.test(residuals(model6), lag=20, type="Ljung-Box")
```

By observing its residual ACF plot, we can observe that most of the residual autocorrelation coefficients in the lag period are within the confidence interval, with only a few lag periods slightly exceeding the confidence interval. And its Ljung-Box test has a p-value greater than 0.05, indicating no significant autocorrelation in the residuals. This indicates that the residuals are mostly white noise, and Model 6 has a good fitting effect.

```{r}
# So we think model6 is the best model.
summary(model6)
```

Based on the results of Model 6, we can write the fitted model equation in backshift notation:
![The fitted model equation](Equation.jpg)


# 4. Neural network autoregression (NNAR) models

```{r}

```


# 5. Assumption checking

```{r}

```


# 6. Forecasting

```{r}

```


# 7. Member contributions

